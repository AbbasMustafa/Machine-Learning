{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nECIB9roXOQ"
      },
      "source": [
        "\n",
        "# **Week 5**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ao4Ey22ocdz"
      },
      "source": [
        "## **DAY 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wowM6IYSopik"
      },
      "source": [
        "##### **Neural Network: Cost Function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSmnGH5Co7eB"
      },
      "source": [
        "###### **Notion**\n",
        "\n",
        "1. **L**: Total Number of layers in the network.\n",
        "2. $S_l$: Number Of units in layer \"*L*\" (exclude Bias unit)\n",
        "3. **K**: Number of Output/Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsBGjzPmsFxS"
      },
      "source": [
        "###### **Cost Function For Nueral Network without Regularization**\n",
        "\n",
        "$J(\\Theta) = -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K} {[y_k^{(i)}\\log(h(\\Theta(x^{(i)}))_k) + [(1-y_k^{(i)})\\log(1-h(\\Theta(x^{(i)}))_k) ]}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23SUZoWHuNXi"
      },
      "source": [
        "###### **Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr83TJkV0UBp"
      },
      "source": [
        "# Cost Function\n",
        "def cost_function(y, h):\n",
        "  m = y.shape[0]\n",
        "\n",
        "  # The great trick - we need to recode the labels as vectors containing only values 0 or 1\n",
        "  y_new = np.zeros((K,m)) # K x m\n",
        "\n",
        "  # One Hot- Encoding\n",
        "  for i in range(m):\n",
        "    y_new[y[i]-1,i] = 1 \n",
        "\n",
        "  J =  (-1/m)* np.sum( (y_new * np.log(h)) + ((1-y_new)*(np.log(1-h))) )\n",
        "  return J"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0voTfsKVocAa",
        "outputId": "9b328d39-581c-4f0b-c7e8-d6660cf9acc3"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def activation_function(z):\n",
        "  return 1/(1+np.exp(-z))\n",
        "\n",
        "features = 2\n",
        "examples = 3\n",
        "hidden_layer = 2\n",
        "K = 4 # no of lbl\n",
        "\n",
        "X = X = np.cos(np.array([[1,2] , [3,4] , [5,6]])) # 3 x 2 (3 examples, 2 features)\n",
        "y = np.array([[4],[2],[3]]) # 4 Classes (no of lbl = 4) (3 x 1)\n",
        "X = np.hstack((np.ones((examples,1)),X)) # 3 x 3 (bias Term)\n",
        "\n",
        "theta1 = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]]) # 2 x 3\n",
        "z_2 = theta1.dot(X.T) # 2 x 3\n",
        "a_2 = activation_function(z_2) # 2 x 3\n",
        "\n",
        "theta2 = np.array([[0.7,1.1,1.5],[0.8,1.2,1.6],[0.9,1.3,1.7],[1.0,1.4,1.8]]) # 4 x 3\n",
        "a_2 = np.hstack((np.ones((a_2.shape[1],1)),a_2.T)) # 3 x 3 (bias Term)\n",
        "z_3 = theta2.dot(a_2.T) # 4 x 3\n",
        "h = activation_function(z_3) # 4 x 3\n",
        "\n",
        "print(f\"Cost without regularization: {cost_function(y,h)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost without regularization: 7.40696985606575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYH2hLtAugT-"
      },
      "source": [
        "## **DAY 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND7yFJoaOywA"
      },
      "source": [
        "###### **Cost Function For Nueral Network with Regularization**\n",
        "\n",
        "$J(\\Theta) = -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K} {[y_k^{(i)}\\log(h(\\Theta(x^{(i)}))_k) + [(1-y_k^{(i)})\\log(1-h(\\Theta(x^{(i)}))_k) ]} + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_{l}}\\sum_{j=1}^{s_{l+1}}(\\Theta^{(l)}_{j,i})^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FSFptTOQmhl"
      },
      "source": [
        "###### **Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgXo5_X6SzXP"
      },
      "source": [
        "# Regularization\n",
        "\n",
        "def regularize(*thetas):\n",
        "  i = 0\n",
        "  t = [0]*len(thetas)\n",
        "  while i < len(thetas):\n",
        "    t[i] = thetas[i][:,1:]\n",
        "    i+=1\n",
        "  \n",
        "  sqrCost = np.array(t,dtype='object')**2\n",
        "  sumCost = 0\n",
        "  for i in sqrCost:\n",
        "    sumCost += np.sum(i)\n",
        "  return sumCost\n",
        "\n",
        "def reg_Cost_Function(y, h, lambda_val, *thetas):\n",
        "  J = cost_function(y, h) + (regularize(*thetas) * (lambda_val/(2*y.shape[0])))\n",
        "  return J"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaY6OnBxuXD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b541606-4532-4fe5-dded-978f1e570284"
      },
      "source": [
        "lambda_val = 4\n",
        "J = reg_Cost_Function(y,h,lambda_val,theta1,theta2)\n",
        "print(f\"Cost with regularization: {J}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost with regularization: 19.473636522732416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHO0LYndutXh"
      },
      "source": [
        "## **DAY 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aKHKMrFvjEA"
      },
      "source": [
        "#### **Back Propogation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx9rUG6bPME6"
      },
      "source": [
        "###### **Intutuion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNGMn3xLvq0k"
      },
      "source": [
        "Backpropagation\" is neural-network terminology for minimizing our cost function, just like what we were doing with gradient descent in logistic and linear regression.\n",
        "\n",
        "$\\min_\\Theta J(\\Theta)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-npuGcrZwdlD"
      },
      "source": [
        "In back propagation we're going to compute for every node:\n",
        "\n",
        "$\\delta_j^{(l)}$ = error in node $j$ in layer $l$\n",
        "\n",
        "For the last layer, we can compute the vector of delta values with:\n",
        "\n",
        "$\\delta^{(L)}=a^{(L)}-y$\n",
        "\n",
        "Where L is our total number of layers and $a^{(L)}$ is the vector of outputs of the activation units for the last layer. So our **\"error values\"** for the last layer are simply the differences of our actual results in the last layer and the correct outputs in y.\n",
        "\n",
        "To get the delta values of the layers before the last layer, we can use an equation that steps us back from right to left:\n",
        "\n",
        "$\\delta^{(l)} = ((\\Theta^{(l)})^{T}\\delta^{(l+1)}).*g'(z^{(l)})$\n",
        "\n",
        "The g-prime derivative terms can also be written out as:\n",
        "\n",
        "$g'(z) = g(z).*(1-g(z))$\n",
        "\n",
        "The full back propagation equation for the inner nodes is then:\n",
        "\n",
        "$\\delta^{(l)} = ((\\Theta^{(l)})^{T}\\delta^{(l+1)}).*a^{(l)}.*(1-a^{(l)})$\n",
        "\n",
        "The delta values of layer l are calculated by multiplying the delta values in the next layer with the theta matrix of layer l. We then element-wise multiply that with a function called g', or g-prime, which is the derivative of the activation function g evaluated with the input values given by $z(l)$.\n",
        "\n",
        "We can compute our partial derivative terms by multiplying our activation values and our error values for each training example t:\n",
        "\n",
        "$\\frac{∂{J(\\Theta)}}{∂{\\Theta^{(l)}_{i,j}}} = \\frac{1}{m}\\sum_{t=1}^{m}a^{(t)(l)}_j*\\delta^{(t)(l+1)}_i$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk0uNYS0PFEM"
      },
      "source": [
        "###### **Agorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0SvRNF_PSF3"
      },
      "source": [
        "* Giving Training set {$(x^{(1)},y^{(1)}) \\dots (x^{(m)},y^{(m)})$}\n",
        "* Set $\\Delta^{(l)}_{i,j} := 0$ for all $(l,i,j)$\n",
        "> For all training example t=1 to m:\n",
        "* Set $a^{(1)} = x^{(t)}$\n",
        "* Perform forward propogation to calculate $a^{(l)}$ for l=2,3...L\n",
        "* Using $y^{(t)}$ compute $\\delta^{(L)} = a^{(L)}-y^{(t)}$\n",
        "* Compute $\\delta^{(L-1)}, \\delta^{(L-2)},\\dots,\\delta^{(2)}$ using $\\delta^{(l)} = ((\\Theta^{(l)})^{T}\\delta^{(l+1)}).*a^{(l)}.*(1-a^{(l)})$\n",
        "* $\\Delta^{(l)}_{i,j} := \\Delta^{(l)}_{i,j} + a_j^{(l)}δ_i^{(l+1)}$ or with vectorization, $Δ^{(l)} := Δ^{(l)} + δ^{(l+1)}(a^{(l)})^T$\n",
        "* $D_{i,j}^{(l)} := \\frac{1}{m}(Δ_{i,j}^{(l)}+λΘ_{i,j}^{(l)})$ If j≠0 (Regularization)\\\n",
        "* $D_{i,j}^{(l)} := \\frac{1}{m}Δ_{i,j}^{(l)}$ If j=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpTEI2yXWTIF"
      },
      "source": [
        "$D_{i,j}^{(l)} = \\frac{∂J(Θ)}{∂Θ_{i,j}^{(l)}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYqzxE7vX4FK"
      },
      "source": [
        "**Note:**\n",
        "\n",
        "More formally, the delta values are actually the derivative of the cost function:\n",
        "\n",
        "$\\delta_j^{(l)} = \\dfrac{\\partial}{\\partial z_j^{(l)}} cost(t)$\n",
        "\n",
        "Recall that our derivative is the slope of a line tangent to the cost function, so the steeper the slope the more incorrect we are."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0i1Vcwufxth"
      },
      "source": [
        "## **DAY 4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c576S7hogfo9"
      },
      "source": [
        "##### **Code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "903KQ9mRgjBx"
      },
      "source": [
        "###### **Data initilization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPk9urTm4VZX"
      },
      "source": [
        "def init_param(Layers):\n",
        "  thetas = [np.random.uniform(low=0,high=1,size=(j,i)) for i,j in zip(Layers[:-1],Layers[1:])]\n",
        "  baises = [np.ones((i,1)) for i in Layers[1:]]\n",
        "  return np.array(thetas,dtype=object),np.array(baises,dtype=object)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHPqsr5JgHvt"
      },
      "source": [
        "###### **Activation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw7-t137gLtG"
      },
      "source": [
        "def activation(z, activationType=\"sigmoid\"):\n",
        "  if activationType == \"sigmoid\":\n",
        "    return sigmoid(z)\n",
        "\n",
        "def sigmoid(z):\n",
        "  z = np.array(z,dtype=np.float)\n",
        "  return 1/(1+np.exp(-z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xplov_mSiqTW"
      },
      "source": [
        "###### **Forward Propogation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDeyQ8ng5UXj"
      },
      "source": [
        "def forward_propogation(X, thetas, baises):\n",
        "  output = [X]\n",
        "  for theta,bais in zip(thetas,baises):\n",
        "    Z = np.dot(theta,output[-1].T) + bais\n",
        "    output.append( activation( Z ,activationType=\"sigmoid\" ).T )\n",
        "\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7kUVYb4kIK1"
      },
      "source": [
        "###### **Backward Propogation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wtT8Y33fNo0"
      },
      "source": [
        "import copy\n",
        "def backward_propagation(y, output, thetas, baises):\n",
        "  m = y.shape[0]\n",
        "  delta = []\n",
        "  dt = []\n",
        "  db = []\n",
        "  \n",
        "  delta = [(output[-1]-y).T * output[-1].T * (1-output[-1].T)]  \n",
        "  for i in range(-len(thetas),-4,-1):\n",
        "    dt.append((1/m) * np.dot(delta[-1], output[i])) \n",
        "    db.append((1/m) * np.sum(delta[-1]))\n",
        "    if(i != -3):\n",
        "      delta.append(np.dot(thetas[-1].T, delta[-1]) * output[i].T * (1-output[i]).T)\n",
        "    \n",
        "\n",
        "  # delta = [(output[-1]-y).T * output[-1].T * (1-output[-1].T)]  # (4x1)]\n",
        "  # dt2 = (1/m) * np.dot(delta[-1], output[-2]) # (1x4) (4x2)\n",
        "  # db2 = (1/m) * np.sum(delta[-1])\n",
        "\n",
        "  # delta.append(np.dot(thetas[-1].T, delta[-1]) * output[-2].T * (1-output[-2]).T) #  (2x1) (1x4)\n",
        "  # dt1 = (1/m) * np.dot(delta[-1], output[-3]) # (2x4) (4x2)\n",
        "  # db1 = (1/m) * np.sum(delta[-1])\n",
        "\n",
        "  der_t = np.array(dt,dtype=object)\n",
        "  der_b = np.array(db,dtype=object)\n",
        "\n",
        "  return der_t[::-1], der_b[::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXXe5NpoqG8V"
      },
      "source": [
        "###### **Update Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qnXC3rFfpXT"
      },
      "source": [
        "def update_param(thetas,baises,dt,db, alpha):\n",
        "\n",
        "  thetas = thetas - alpha*dt\n",
        "  baises = baises - alpha*db\n",
        "  return thetas,baises"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNi4IQYyQHHl"
      },
      "source": [
        "###### **Cost**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFI9T5NoQKJp"
      },
      "source": [
        "def Cost(h,y):\n",
        "  m = y.shape[0]\n",
        "  return (-1/m)* np.sum( (y * np.log(h)) + ((1-y)*(np.log(1-h))) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOIPHCMHrKMv"
      },
      "source": [
        "###### **Gradient Descent**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAeLnB5ogFVf"
      },
      "source": [
        "def gradient_descent(X, y, alpha, itr, Layers):\n",
        "    J=[]\n",
        "    thetas,baises = init_param(Layers)\n",
        "    for i in range(itr):\n",
        "      output = forward_propogation(X,thetas,baises)\n",
        "      dt,db = backward_propagation(y, output, thetas, baises)\n",
        "      thetas,baises = update_param(thetas,baises,dt,db,alpha)\n",
        "      J.append(Cost(output[-1],y))\n",
        "      if i % 10000 == 0:\n",
        "        print(f\"iteration: {i}\")\n",
        "        print(f\"Cost Value: {J[-1]}\")\n",
        "    plt.plot(range(itr),J,'g-')\n",
        "    plt.grid(True)\n",
        "    plt.show(True)\n",
        "    \n",
        "    return thetas,baises"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptKUmHIygGuj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "db37d9df-83f3-484f-de24-f55aa522be06"
      },
      "source": [
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[1],[0],[0],[1]])\n",
        "\n",
        "thetas,baises = gradient_descent(X, y,alpha=0.5,itr=100000,Layers=[2,3,1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration: 0\n",
            "Cost Value: 1.1522121689838882\n",
            "iteration: 10000\n",
            "Cost Value: 0.044474999131574364\n",
            "iteration: 20000\n",
            "Cost Value: 0.0260564409294183\n",
            "iteration: 30000\n",
            "Cost Value: 0.01995605823099309\n",
            "iteration: 40000\n",
            "Cost Value: 0.016702509199772973\n",
            "iteration: 50000\n",
            "Cost Value: 0.014616133636648876\n",
            "iteration: 60000\n",
            "Cost Value: 0.013137562215807957\n",
            "iteration: 70000\n",
            "Cost Value: 0.012021509521447198\n",
            "iteration: 80000\n",
            "Cost Value: 0.011141636585874276\n",
            "iteration: 90000\n",
            "Cost Value: 0.010425520173262995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYwaKEtOn43N"
      },
      "source": [
        "## **DAY 4**"
      ]
    }
  ]
}